{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a34f14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82f6c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16,preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical,plot_model\n",
    "from tensorflow.keras.layers import Input,Dense,LSTM,Embedding,Dropout,add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd3c939d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = VGG16()\n",
    "model = Model(inputs = model.inputs,outputs = model.layers[-2].output)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32ecb471",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/Users/91887/imageClassification/archive (1)'\n",
    "WORKING_DIR = '/Users/91887/imageClassification'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d82f45",
   "metadata": {},
   "source": [
    "Extact Image Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61b7d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c60bc1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50ba9ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\91887\\AppData\\Local\\Temp/ipykernel_4152/3614151985.py:4: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.95\n",
    "config.gpu_options.visible_device_list = \"0\"\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eed06f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6efbce6bb8894eecbb9ebe642508bea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = {}\n",
    "directory = os.path.join(BASE_DIR,'Images')\n",
    "\n",
    "for img_name in tqdm(os.listdir(directory)):\n",
    "    #load the image from file \n",
    "    img_path = directory+'/'+img_name\n",
    "    image = load_img(img_path,target_size = (224,224))\n",
    "    #image pixel to numpy array \n",
    "    image = img_to_array(image)\n",
    "    #reshapre data for model\n",
    "    image =  image.reshape((1,image.shape[0],image.shape[1],image.shape[2]))\n",
    "    #preprocess image for vgg\n",
    "    image = preprocess_input(image)\n",
    "    #Extract the Features\n",
    "    feature = model.predict(image,verbose=0)\n",
    "    #get Image ID\n",
    "    image_id = img_name.split('.')[0]\n",
    "    #store the Features\n",
    "    features[image_id] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a865123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Features in pickle \n",
    "\n",
    "pickle.dump(features,open(os.path.join(WORKING_DIR,'features.pkl'),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6f2233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Features from pickle \n",
    "\n",
    "with open(os.path.join(WORKING_DIR,'features.pkl'),'rb') as f:\n",
    "    features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6dd1533",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(BASE_DIR,'captions.txt'),'r') as f:\n",
    "    next(f)\n",
    "    captions_doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d13d1f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14a6fd2165c44558f89c22b3a9a7ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create mapping of images to captions\n",
    "\n",
    "# Creating Dict name mapping \n",
    "\n",
    "mapping = {}\n",
    "\n",
    "#process lines \n",
    "\n",
    "for line in tqdm(captions_doc.split('\\n')):\n",
    "    #Split the line by comma\n",
    "    tokens = line.split(',')\n",
    "    if len(line)<2:\n",
    "        continue\n",
    "    image_id, caption = tokens[0], tokens[1:]\n",
    "    #remove exe from image_id\n",
    "    image_id = image_id.split('.')[0]\n",
    "    #convert caption list into the string \n",
    "    caption =\" \".join(caption)\n",
    "    #Create a list \n",
    "    if image_id not in mapping:\n",
    "        mapping[image_id] = []\n",
    "    mapping[image_id].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "979e54d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8091"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f0952f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0ad2db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(mapping):\n",
    "    for key,captions in mapping.items():\n",
    "        for i in range(len(captions)):\n",
    "            caption = captions[i]\n",
    "            #Preprocessing steps\n",
    "            caption = caption.lower()\n",
    "            caption = caption.replace('[^A-Za-z]', '')  #Delete special character and digit\n",
    "            caption = caption.replace('\\s+',' ')\n",
    "            # add Start and End tags to the caption\n",
    "            caption = 'start ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' end'\n",
    "            captions[i] = caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "10f593f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A child in a pink dress is climbing up a set of stairs in an entry way .', 'A girl going into a wooden building .', 'A little girl climbing into a wooden playhouse .', 'A little girl climbing the stairs to her playhouse .', 'A little girl in a pink dress going into a wooden cabin .']\n"
     ]
    }
   ],
   "source": [
    "value = mapping[\"1000268201_693b08cb0e\"]\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eba16ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d39d59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['start child in pink dress is climbing up set of stairs in an entry way end',\n",
       " 'start girl going into wooden building end',\n",
       " 'start little girl climbing into wooden playhouse end',\n",
       " 'start little girl climbing the stairs to her playhouse end',\n",
       " 'start little girl in pink dress going into wooden cabin end']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "340069db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Tokenizer for getting the index of the corressponding word \n",
    "# Create vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "212a81b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions = []\n",
    "for key in mapping:\n",
    "    for caption in mapping[key]:\n",
    "        all_captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "063cf31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40455"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3542bc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['start child in pink dress is climbing up set of stairs in an entry way end',\n",
       " 'start girl going into wooden building end',\n",
       " 'start little girl climbing into wooden playhouse end',\n",
       " 'start little girl climbing the stairs to her playhouse end',\n",
       " 'start little girl in pink dress going into wooden cabin end',\n",
       " 'start black dog and spotted dog are fighting end',\n",
       " 'start black dog and tri-colored dog playing with each other on the road end',\n",
       " 'start black dog and white dog with brown spots are staring at each other in the street end',\n",
       " 'start two dogs of different breeds looking at each other on the road end',\n",
       " 'start two dogs on pavement moving toward each other end']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "faa79e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1f1df76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "voca_size = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "faa1cbee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8483"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voca_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ab04f95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get max length of the caption available\n",
    "max_len = max(len(caption.split())for caption in all_captions)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029a2271",
   "metadata": {},
   "source": [
    "Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b0a08640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6472"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ids = list(mapping.keys())\n",
    "split = int(len(image_ids)*0.80)\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ea122db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = image_ids[:split]\n",
    "test = image_ids[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "53bbfb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator (data_keys,mapping,features,tokenizer,max_length,voca_size,batch_size):\n",
    "    #looping over images \n",
    "    X1,X2,y = list(),list(),list()\n",
    "    n =0\n",
    "    while 1:\n",
    "        for key in data_keys:\n",
    "            n+=1;\n",
    "            captions = mapping[key]\n",
    "            for caption in captions:\n",
    "                #Encode the Sequence\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                #Split the sequence into X y pairs \n",
    "                for i in range(1,len(seq)):\n",
    "                    in_seq,out_seq = seq[:i],seq[i]\n",
    "                    #pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq],maxlen=max_len)[0]\n",
    "                    #encode output seq \n",
    "                    out_seq = to_categorical([out_seq],num_classes = voca_size)[0]\n",
    "                    \n",
    "                    #Store the seq\n",
    "                    X1.append(features[key][0])\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "                    \n",
    "            if n == batch_size:\n",
    "                X1,X2,y = np.array(X1),np.array(X2),np.array(y)\n",
    "                yield [X1,X2],y\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n =0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e487e8a8",
   "metadata": {},
   "source": [
    "Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d39ceb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1ff80dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)          [(None, 35)]         0           []                               \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)          [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 35, 256)      2171648     ['input_12[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096)         0           ['input_11[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 35, 256)      0           ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 256)          1048832     ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_4 (LSTM)                  (None, 256)          525312      ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 256)          0           ['dense_12[0][0]',               \n",
      "                                                                  'lstm_4[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 256)          65792       ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 8483)         2180131     ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,991,715\n",
      "Trainable params: 5,991,715\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Encoder Model\n",
    "#Image Feature layers\n",
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.4)(inputs1)\n",
    "fe2 = Dense(256,activation='relu')(fe1)\n",
    "#Sequence feature layer\n",
    "inputs2 = Input(shape =(max_len,))\n",
    "se1 = Embedding(voca_size,256,mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.4)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "#Decoder Model\n",
    "decoder1 = add([fe2,se3])\n",
    "decoder2 = Dense(256,activation = 'relu')(decoder1)\n",
    "outputs = Dense(voca_size,activation='softmax')(decoder2)\n",
    "\n",
    "model = Model(inputs=(inputs1,inputs2),outputs = outputs)\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90840689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 601s 6s/step - loss: 5.6646\n",
      "101/101 [==============================] - 849s 8s/step - loss: 4.6210\n",
      "101/101 [==============================] - 1404s 14s/step - loss: 3.9623\n",
      "101/101 [==============================] - 1080s 11s/step - loss: 3.6237\n",
      "101/101 [==============================] - 1091s 11s/step - loss: 3.4052\n",
      "101/101 [==============================] - 1085s 11s/step - loss: 3.2402\n",
      "101/101 [==============================] - 1558s 15s/step - loss: 3.0997\n",
      "101/101 [==============================] - 1481s 15s/step - loss: 2.9825\n",
      "101/101 [==============================] - 983s 10s/step - loss: 2.8867\n",
      "101/101 [==============================] - 918s 9s/step - loss: 2.7980\n",
      "101/101 [==============================] - 1619s 16s/step - loss: 2.7285\n",
      " 39/101 [==========>...................] - ETA: 2:06:48 - loss: 2.6706"
     ]
    }
   ],
   "source": [
    "#Train the Model\n",
    "epochs = 15\n",
    "batch_size = 64\n",
    "steps = len(train)//batch_size\n",
    "\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(train,mapping,features,tokenizer,max_len,voca_size,batch_size)\n",
    "    model.fit(generator,epochs=1,steps_per_epoch = steps,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dc6de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the Model \n",
    "model.save(WORKING_DIR+'/best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be151508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Captions for the Image \n",
    "#Index to Word\n",
    "\n",
    "def idx_to_word(integer,tokenizer):\n",
    "    for word,index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word \n",
    "         return None   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b26791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Caption for the Image \n",
    "def predict_caption(model,image,tokenizer,max_length):\n",
    "    #Add start tag for generation process\n",
    "    in_text = 'start'\n",
    "    #iterate over max-length of sequence\n",
    "    for i in range(max_length):\n",
    "        #Encode input sequences\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        #Pad the Seequence\n",
    "        sequence = pad_sequences([sequence], max_length)\n",
    "        #predict next word \n",
    "        yhat = model.predict([image,sequence],verbose = 0)\n",
    "        #Get index with High Probability\n",
    "        yhat = np.argmax(yhat)\n",
    "        #Convert index to word\n",
    "        word = idx_to_word(yhat,tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        #Append word as input for generating next word\n",
    "        in_text += \" \"+word\n",
    "        #Stop if we reach end Tag\n",
    "        if word == 'end':\n",
    "            break\n",
    "        return in_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97189d4",
   "metadata": {},
   "source": [
    "Find BLEU Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa894da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aeaa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate with Test Data\n",
    "actual,predicted = list(),list()\n",
    "\n",
    "for key in tqdm(test):\n",
    "    #get Actual Caption\n",
    "    captions = mapping[key]\n",
    "    #predict the caption of the Image \n",
    "    y_pred = predict_caption(model,features[key],tokenizer,max_length)\n",
    "    #Split into words \n",
    "    y_pred = y_pred.split()\n",
    "    actual_captions = [caption.split() for caption in captions]\n",
    "    actual.append(actual_captions)\n",
    "    predicted.append(y_pred)\n",
    "\n",
    "#Calculate BLEU score\n",
    "print(\"BLEU-1 : %f\"%corpus_bleu(actual,predicted,weights=(1.0,0,0,0)))\n",
    "print(\"BLEU-2 : %f\"%corpus_bleu(actual,predicted,weights=(0.5,0.5,0,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed7e60f",
   "metadata": {},
   "source": [
    "Visualize the Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14db5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "def generate_caption(image_name):\n",
    "    image_name = \"1022454332_6af2c1449a.jpg\"\n",
    "    image_id = image_name.split('.')[0]\n",
    "    image_path = os.path.join(BASE_DIR,'Images',image_name)\n",
    "    image = Image.open(img_path)\n",
    "    captions = mapping[image_id]\n",
    "    print('------------------Actual------------------')\n",
    "    for caption in captions:\n",
    "        print(caption)\n",
    "    y_pred = predict_caption(model,features[image_id],tokenizer,max_length)\n",
    "    print('------------------Predicted------------------')\n",
    "    print(y_pred)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9516ae9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ae9630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ecefb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d3ef56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9087985d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
